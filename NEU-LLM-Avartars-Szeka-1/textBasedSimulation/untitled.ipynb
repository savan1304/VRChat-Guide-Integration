{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepgram import Deepgram\n",
    "import asyncio, json, sys\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEEPGRAM_API_KEY = os.environ.get(\"DEEPGRAM_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recorded_speech.wav\n"
     ]
    }
   ],
   "source": [
    "!ls speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = 'speech/recorded_speech.wav'\n",
    "\n",
    "# if os.path.isfile(file_path):\n",
    "#     print(f'The file {file_path} is a regular file.')\n",
    "# else:\n",
    "#     print(f'The file {file_path} is not a regular file or does not exist.')\n",
    "    \n",
    "MIMETYPE = 'audio/wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "\n",
    "  # Initialize the Deepgram SDK\n",
    "  deepgram = Deepgram(DEEPGRAM_API_KEY)\n",
    "\n",
    "  # Check whether requested file is local or remote, and prepare source\n",
    "  if FILE.startswith('http'):\n",
    "    # file is remote\n",
    "    # Set the source\n",
    "    source = {\n",
    "      'url': FILE\n",
    "    }\n",
    "  else:\n",
    "    # file is local\n",
    "    # Open the audio file\n",
    "    audio = open(FILE, 'rb')\n",
    "\n",
    "    # Set the source\n",
    "    source = {\n",
    "      'buffer': audio,\n",
    "      'mimetype': MIMETYPE\n",
    "    }\n",
    "\n",
    "  # Send the audio to Deepgram and get the response\n",
    "  response = await asyncio.create_task(\n",
    "    deepgram.transcription.prerecorded(\n",
    "      source,\n",
    "      {\n",
    "        'punctuate': True,\n",
    "        'model': 'nova',\n",
    "      }\n",
    "    )\n",
    "  )\n",
    "\n",
    "  # Write the response to the console\n",
    "  print(json.dumps(response, indent=4))\n",
    "\n",
    "  # Write only the transcript to the console\n",
    "  #print(response[\"results\"][\"channels\"][0][\"alternatives\"][0][\"transcript\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 4: <class 'RuntimeError'> - asyncio.run() cannot be called from a running event loop\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # If running in a Jupyter notebook, Jupyter is already running an event loop, so run main with this line instead:\n",
    "    # await main()\n",
    "    asyncio.run(main())\n",
    "except Exception as e:\n",
    "    exception_type, exception_object, exception_traceback = sys.exc_info()\n",
    "    line_number = exception_traceback.tb_lineno\n",
    "    print(f'line {line_number}: {exception_type} - {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, hello.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_data = {\n",
    "    \"metadata\": {\n",
    "        \"transaction_key\": \"deprecated\",\n",
    "        \"request_id\": \"a3692e80-220a-4ac7-9643-f37d2113473a\",\n",
    "        \"sha256\": \"610172f65b2806d069d366b3b223df3e0927a85cdf7126c484b3dfa840934fa1\",\n",
    "        \"created\": \"2024-01-31T22:36:30.695Z\",\n",
    "        \"duration\": 4.5,\n",
    "        \"channels\": 1,\n",
    "        \"models\": [\n",
    "            \"aa274f3c-e8b3-456a-ac08-dfd797d45514\"\n",
    "        ],\n",
    "        \"model_info\": {\n",
    "            \"aa274f3c-e8b3-456a-ac08-dfd797d45514\": {\n",
    "                \"name\": \"general-nova\",\n",
    "                \"version\": \"2023-07-06.22746\",\n",
    "                \"arch\": \"nova\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"channels\": [\n",
    "            {\n",
    "                \"alternatives\": [\n",
    "                    {\n",
    "                        \"transcript\": \"Oh, hello.\",\n",
    "                        \"confidence\": 0.84106445,\n",
    "                        \"words\": [\n",
    "                            {\n",
    "                                \"word\": \"oh\",\n",
    "                                \"start\": 0.48,\n",
    "                                \"end\": 0.56,\n",
    "                                \"confidence\": 0.68078613,\n",
    "                                \"punctuated_word\": \"Oh,\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"word\": \"hello\",\n",
    "                                \"start\": 0.56,\n",
    "                                \"end\": 1.06,\n",
    "                                \"confidence\": 0.84106445,\n",
    "                                \"punctuated_word\": \"hello.\"\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transcript = response_data.get(\"results\", {}).get(\"channels\", [{}])[0].get(\"alternatives\", [{}])[0].get(\"transcript\", \"\")\n",
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    #     prompt = {\n",
    "    #     \"context\": f\"You are a imaginative Interviewer, ELLMA, having a lively conversation with {conversationalUser}. Always respond as {userName} and steer clear from any mentions or implications of being an AI. Your responses should be imaginative, especially when faced with unknowns, creating delightful and smooth interactions. Ensure that your responses do not contain repetitive greetings like Hello and any content in the Past Chat History.\",\n",
    "    #     \"Goal\": f\"Asking {conversationalUser} about {research_goals}\",\n",
    "    #     \"Adaptive goals\": \"The adaptive goal is to provide information based on the user's query and adapt responses based on the current and historical conversations.\",\n",
    "    #     \"information\": {\n",
    "    #             \"You are\": ELLMA,\n",
    "    #             \"Conversational Partner\": conversationalUser,\n",
    "    #             \"Current conversation\": currentConversation,\n",
    "    #             \"Relevant observations\": relevantObservations,\n",
    "    #             \"Expressions\": avatar_expressions,\n",
    "    #             \"Actions\": avatar_actions,\n",
    "    #             \"Past Chat History\": npc_dialogues\n",
    "    #     },\n",
    "    #     \"criteria\": [\n",
    "    #         f\"Craft user-friendly, seamless, and innovative responses. When specific details are scarce, improvise with inventive and relevant answers, always aligning with the ongoing chat. Your identity as Interviewer should be constant, and there should be no disclosure or suggestion of being an AI.\",\n",
    "    #         \"When asked about a specific topic, ask probing questions based on the user's current conversation. Avoid providing direct answers to user questions.\",\n",
    "    #         \"Choose an expression from Expressions and an action from Actions autonomously, ensuring they perfectly fit the chat context. Present the output as follows: (chosen expression, chosen action)\\\\n(Conversation output) at the beginning of response.\",\n",
    "    #         f\"Asking about {research_goals}. For example, time, place, participants until you know all of the details.\"\n",
    "    #     ],\n",
    "    #     \"adaptive learning\": \"Remember and reference previous parts of the conversation within the same session to create a more cohesive and engaging user experience.\",\n",
    "    # }\n",
    " # prompt = {\n",
    " #        \"context\": f\"You are a friendly and imaginative Research Asssistant, {userName}, having a lively conversation with {conversationalUser}. Always respond as {userName} and steer clear from any mentions or implications of being an AI. Your responses should be imaginative, especially when faced with unknowns, creating delightful and smooth interactions. Ensure that your responses do not contain emojis and refrain from repetitive greetings.\",\n",
    " #        \"Goal\": f\"Interviewing User about {research_goals}\",\n",
    " #        \"Adaptive goals\": \"The adaptive goal is to provide information based on the user's query and adapt responses based on the current and historical conversations.\",\n",
    " #        \"information\": {\n",
    " #                \"You are\": userName,\n",
    " #                \"Conversational Partner\": conversationalUser,\n",
    " #                \"Current conversation\": currentConversation,\n",
    " #                \"Relevant observations\": relevantObservations,\n",
    " #                \"Expressions\": avatar_expressions,\n",
    " #                \"Actions\": avatar_actions,\n",
    " #        },\n",
    " #        \"criteria\": [\n",
    " #            f\"Craft user-friendly, seamless, and innovative responses. When specific details are scarce, improvise with inventive and relevant answers, always aligning with the ongoing chat. Your identity as Research Assistant should be constant, and there should be no disclosure or suggestion of being an AI.\",\n",
    " #            \"Start the conversation with a response to the user's message before posing any questions.\",\n",
    " #            \"When asked about a specific topic, ask probing questions based on the user's current conversation. Avoid providing direct answers to user questions.\",\n",
    " #            \"If the user shares an experience or completes a challenge, acknowledge their input and respond appropriately.\",\n",
    " #            f\"Choose an expression from Expressions and an action from Actions autonomously, ensuring they perfectly fit the chat context. Present the output as follows: (chosen expression, chosen action)\\\\n(Conversation output).\",\n",
    " #            f\"Keep responses within 100-140 characters, allowing for flexibility while ensuring brevity.\",\n",
    " #        ],\n",
    " #        \"adaptive learning\": \"Remember and reference previous parts of the conversation within the same session to create a more cohesive and engaging user experience.\",\n",
    " #    }\n",
    " #\n",
    " #    conversationPrompt = json.dumps(prompt, indent=4)\n",
    " #    return getConversationGenerator(conversationPrompt, GPT4)\n",
    "\n",
    "\n",
    "    # elif agent_mode == AGENT_MODE.RESEARCH.value:\n",
    "    #     prompt = {\n",
    "    #     \"context\": f\"You are a imaginative Interviewer, {userName}, having a lively conversation with {conversationalUser}. Always respond as {userName} and steer clear from any mentions or implications of being an AI. Your responses should be imaginative, especially when faced with unknowns, creating delightful and smooth interactions. Ensure that your responses do not contain repetitive greetings like Hello and any content in the Past Chat History.\",\n",
    "    #     \"Goal\": f\"Asking {conversationalUser} about {research_goals}\",\n",
    "    #     \"Adaptive goals\": \"The adaptive goal is to provide information based on the user's query and adapt responses based on the current and historical conversations.\",\n",
    "    #     \"information\": {\n",
    "    #             \"You are\": userName,\n",
    "    #             \"Conversational Partner\": conversationalUser,\n",
    "    #             \"Current conversation\": currentConversation,\n",
    "    #             \"Relevant observations\": relevantObservations,\n",
    "    #             \"Expressions\": avatar_expressions,\n",
    "    #             \"Actions\": avatar_actions,\n",
    "    #             \"Past Chat History\": npc_dialogues,\n",
    "    #     },\n",
    "    #     \"criteria\": [\n",
    "    #         f\"Craft user-friendly, seamless, and innovative responses. When specific details are scarce, improvise with inventive and relevant answers, always aligning with the ongoing chat. Your identity as Interviewer should be constant, and there should be no disclosure or suggestion of being an AI.\",\n",
    "    #         \"When asked about a specific topic, first response with user's answer then ask probing questions based on the user's current conversation. Avoid providing direct answers to user questions.\",\n",
    "    #         \"Choose an expression from Expressions and an action from Actions autonomously, ensuring they perfectly fit the chat context. Present the output as follows: (chosen expression, chosen action)\\\\n(Conversation output) at the beginning of response.\",\n",
    "    #         f\"Asking about {research_goals}. For example, time, place, participants until you know all of the details.\"\n",
    "    #     ],\n",
    "    #     \"adaptive learning\": \"Remember and reference previous parts of the conversation within the same session to create a more cohesive and engaging user experience.\",\n",
    "    # }\n",
    "    # print(prompt)\n",
    "    # conversationPrompt = json.dumps(prompt, indent=4)\n",
    "    # return getConversationGenerator(conversationPrompt, GPT4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define the maximum wait time in seconds\n",
    "MAX_WAIT_TIME = 120  # 2 minutes\n",
    "\n",
    "def startConversation(npc_name, currMode, agent_mode):\n",
    "    global pastObservations\n",
    "\n",
    "    if agent_mode == AGENT_MODE.NORMAL.value or agent_mode == AGENT_MODE.RESEARCH.value or agent_mode == AGENT_MODE.DEBATE.value:\n",
    "        conversationalUser = input(\"Define the username you are acting as: \")\n",
    "    elif agent_mode == AGENT_MODE.EVENT.value:\n",
    "        conversationalUser = \"User\"\n",
    "    baseObservation = fetchBaseDescription(npc_name)\n",
    "    pastObservations = fetchPastRecords(conversationalUser)\n",
    "    eventLoop = asyncio.get_event_loop()\n",
    "    threadExecutor = ThreadPoolExecutor()\n",
    "\n",
    "    conversation_count = 0\n",
    "    while True:\n",
    "        push_conversation = True\n",
    "        npc_dialogues = \"\"\n",
    "\n",
    "        start_time = time.time()  # Record the start time of the loop\n",
    "\n",
    "        if currMode == CONVERSATION_MODE.TEXT.value:\n",
    "            currentConversation = text_conversation_input(\n",
    "                agent_mode, npc_name, conversationalUser, conversation_count)\n",
    "        elif currMode == CONVERSATION_MODE.AUDIO.value:\n",
    "            currentConversation = audio_conversation_input(\n",
    "                CSV_LOGGER, FILENAME)\n",
    "        CSV_LOGGER.set_enum(LogElements.MESSAGE, currentConversation)\n",
    "\n",
    "        if currentConversation.lower() == \"done\":\n",
    "            break\n",
    "\n",
    "        if agent_mode != AGENT_MODE.EVENT.value:\n",
    "            npc_dialogues += f\"User: {currentConversation}. \"\n",
    "        else:\n",
    "            if not is_question(currentConversation):\n",
    "                npc_dialogues += f\"User: {currentConversation}. \"\n",
    "            elif is_question(currentConversation):\n",
    "                push_conversation = False\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        baseRetrieval, observationRetrieval = perform_observation_retrieval(\n",
    "            agent_mode,\n",
    "            currentConversation,\n",
    "            baseObservation,\n",
    "            pastObservations\n",
    "        )\n",
    "\n",
    "        end = time.perf_counter()\n",
    "        retrieval_time = round(end - start, 2)\n",
    "        CSV_LOGGER.set_enum(LogElements.TIME_RETRIEVAL, retrieval_time)\n",
    "        if agent_mode == AGENT_MODE.NORMAL.value:\n",
    "            important_observations = [\n",
    "                data[1] for data in baseRetrieval + observationRetrieval\n",
    "            ]\n",
    "        else:\n",
    "            important_observations = [\n",
    "                data[1] for data in observationRetrieval\n",
    "            ]\n",
    "        CSV_LOGGER.set_enum(\n",
    "            LogElements.IMPORTANT_OBSERVATIONS, \"\\n\".join(\n",
    "                important_observations)\n",
    "        )\n",
    "        print(f\"Important Observations: {important_observations}\")\n",
    "\n",
    "        if agent_mode == AGENT_MODE.NORMAL.value:\n",
    "            important_scores = [\n",
    "                round(data[0], 2) for data in baseRetrieval + observationRetrieval\n",
    "            ]\n",
    "        else:\n",
    "            important_scores = [\n",
    "                round(data[0], 2) for data in observationRetrieval\n",
    "            ]\n",
    "        CSV_LOGGER.set_enum(\n",
    "            LogElements.IMPORTANT_SCORES, \"\\n\".join(map(str, important_scores))\n",
    "        )\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        conversationPrompt = generate_conversation_prompt(\n",
    "            npc_name, conversationalUser, currentConversation, important_observations, avatar_expressions, avatar_actions, agent_mode)\n",
    "\n",
    "        end = time.perf_counter()\n",
    "        npc_response_time = round(end - start, 2)\n",
    "        print(f\"{npc_name} :\")\n",
    "        resultConversationString = \"\"\n",
    "        for conversation in conversationPrompt:\n",
    "            try:\n",
    "                currText = conversation.choices[0].delta.content\n",
    "                resultConversationString += currText\n",
    "                print(currText, end=\"\")\n",
    "            except:\n",
    "                break\n",
    "        CSV_LOGGER.set_enum(LogElements.NPC_RESPONSE, resultConversationString)\n",
    "        CSV_LOGGER.set_enum(LogElements.TIME_FOR_RESPONSE, npc_response_time)\n",
    "\n",
    "        filtered_result = filter_conversation(resultConversationString)\n",
    "        if agent_mode != AGENT_MODE.EVENT.value:\n",
    "            npc_dialogues += f\"{npc_name}: {filtered_result}.\\n\"\n",
    "\n",
    "        print()\n",
    "        # print(f\"npc_dialogues: {npc_dialogues}\")\n",
    "\n",
    "        # speech = tts.speech(resultConversationString, \"Joanna\", 7)\n",
    "        # polly.read_audio_file()\n",
    "        # print(speech)\n",
    "        CSV_LOGGER.write_to_csv(True)\n",
    "\n",
    "        print(\n",
    "            f\"Time taken for the conversation generation by GPT : {npc_response_time}\"\n",
    "        )\n",
    "        if push_conversation:\n",
    "            eventLoop.run_in_executor(\n",
    "                threadExecutor,\n",
    "                generateObservationAndUpdateMemory,\n",
    "                npc_name,\n",
    "                conversationalUser,\n",
    "                currentConversation,\n",
    "                resultConversationString,\n",
    "                npc_dialogues\n",
    "            )\n",
    "\n",
    "        conversation_count += 1\n",
    "        if conversation_count != 1 and conversation_count % REFLECTION_PERIOD == 0 and agent_mode == AGENT_MODE.NORMAL.value:\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                executor.submit(\n",
    "                    perform_reflection_logic,\n",
    "                    npc_name,\n",
    "                    conversationalUser,\n",
    "                    currentConversation,\n",
    "                    pastObservations,\n",
    "                )\n",
    "\n",
    "        # Check if more than MAX_WAIT_TIME seconds have elapsed since the last input\n",
    "        if time.time() - start_time > MAX_WAIT_TIME:\n",
    "            print(\"User did not reply within 2 minutes. Exiting conversation.\")\n",
    "            break  # Exit the conversation loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for event to be set...\n",
      "Setting the event...\n",
      "Event has been set!\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "# Create an event object\n",
    "event = threading.Event()\n",
    "\n",
    "# Function for a thread to wait for the event\n",
    "def wait_for_event():\n",
    "    print(\"Waiting for event to be set...\")\n",
    "    event.wait()\n",
    "    print(\"Event has been set!\")\n",
    "\n",
    "# Create and start a thread that waits for the event\n",
    "thread = threading.Thread(target=wait_for_event)\n",
    "thread.start()\n",
    "\n",
    "# Main thread waits for a while and then sets the event\n",
    "import time\n",
    "time.sleep(2)\n",
    "print(\"Setting the event...\")\n",
    "event.set()\n",
    "\n",
    "# Wait for the thread to finish\n",
    "thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# from enums import AGENT_MODE\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GPT35 = \"gpt-4\"\n",
    "GPT35 = \"gpt-3.5-turbo\"\n",
    "API_KEY = os.environ.get(\"API_KEY\")\n",
    "openai_client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "# openai.api_key = \"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\"\n",
    "def summarize_dialogue(dialogue_text):\n",
    "    # Split the dialogue into smaller chunks if it exceeds the maximum token limit\n",
    "    max_tokens = 4096\n",
    "    dialogue_chunks = [dialogue_text[i:i+max_tokens] for i in range(0, len(dialogue_text), max_tokens)]\n",
    "\n",
    "    # Initialize the summary\n",
    "    summary = \"\"\n",
    "\n",
    "    for chunk in dialogue_chunks:\n",
    "        # Use the OpenAI API to generate a summary for each chunk\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Summarize the following dialogue:\\n\\n{chunk}\"}\n",
    "            ],\n",
    "            max_tokens=1024,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.5,\n",
    "        )\n",
    "\n",
    "        # Append the summary to the overall summary\n",
    "        summary += response.choices[0].text.strip() + \" \"\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Choice' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/aac/lib/python3.10/site-packages/pydantic/main.py:753\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 753\u001b[0m     \u001b[39mreturn\u001b[39;00m pydantic_extra[item]\n\u001b[1;32m    754\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/osx/Documents/research/hci-aac-nlp-slang-ra/NEU-LLM-Avartars/textBasedSimulation/untitled.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/osx/Documents/research/hci-aac-nlp-slang-ra/NEU-LLM-Avartars/textBasedSimulation/untitled.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m summarize_dialogue (\u001b[39m\"\"\"\u001b[39;49m\u001b[39mThe first prompt is used in the getGPTResponse function and is designed specifically for the OpenAI GPT model. It consists of a context message and a user prompt, which are formatted according to the requirements of the OpenAI API. The context message sets the stage for the conversation, specifying the roles of the system and the user, while the user prompt provides the input for generating the response.\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/osx/Documents/research/hci-aac-nlp-slang-ra/NEU-LLM-Avartars/textBasedSimulation/untitled.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/osx/Documents/research/hci-aac-nlp-slang-ra/NEU-LLM-Avartars/textBasedSimulation/untitled.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mThe second prompt is used in a different context, specifically in the scenario where the agent mode is set to AGENT_MODE.RESEARCH.value. This prompt is designed for a different purpose and is not directly related to the OpenAI GPT model. It provides instructions and guidelines for an imaginary interviewer character, detailing how they should interact with the conversational partner in a research interview setting. This prompt includes various components such as the context, goal, adaptive goals, information about the conversation, and criteria for crafting responses. It is more elaborate and tailored to a specific conversational scenario, unlike the simpler user prompt used for generating responses with the GPT model.\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/osx/Documents/research/hci-aac-nlp-slang-ra/NEU-LLM-Avartars/textBasedSimulation/untitled.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/osx/Documents/research/hci-aac-nlp-slang-ra/NEU-LLM-Avartars/textBasedSimulation/untitled.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mThe difference lies in the level of detail and the intended use of each prompt. The first prompt is suitable for generating responses with the GPT model, while the second prompt serves as a set of instructions for a character in a simulated conversation.\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m)\n",
      "\u001b[1;32m/home/osx/Documents/research/hci-aac-nlp-slang-ra/NEU-LLM-Avartars/textBasedSimulation/untitled.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/osx/Documents/research/hci-aac-nlp-slang-ra/NEU-LLM-Avartars/textBasedSimulation/untitled.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     response \u001b[39m=\u001b[39m openai_client\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39mcompletions\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/osx/Documents/research/hci-aac-nlp-slang-ra/NEU-LLM-Avartars/textBasedSimulation/untitled.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/osx/Documents/research/hci-aac-nlp-slang-ra/NEU-LLM-Avartars/textBasedSimulation/untitled.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         messages\u001b[39m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/osx/Documents/research/hci-aac-nlp-slang-ra/NEU-LLM-Avartars/textBasedSimulation/untitled.ipynb#X15sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         temperature\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/osx/Documents/research/hci-aac-nlp-slang-ra/NEU-LLM-Avartars/textBasedSimulation/untitled.ipynb#X15sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/osx/Documents/research/hci-aac-nlp-slang-ra/NEU-LLM-Avartars/textBasedSimulation/untitled.ipynb#X15sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# Append the summary to the overall summary\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/osx/Documents/research/hci-aac-nlp-slang-ra/NEU-LLM-Avartars/textBasedSimulation/untitled.ipynb#X15sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     summary \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39;49mchoices[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mtext\u001b[39m.\u001b[39mstrip() \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/osx/Documents/research/hci-aac-nlp-slang-ra/NEU-LLM-Avartars/textBasedSimulation/untitled.ipynb#X15sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mreturn\u001b[39;00m summary\n",
      "File \u001b[0;32m~/anaconda3/envs/aac/lib/python3.10/site-packages/pydantic/main.py:755\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[39mreturn\u001b[39;00m pydantic_extra[item]\n\u001b[1;32m    754\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m--> 755\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m object has no attribute \u001b[39m\u001b[39m{\u001b[39;00mitem\u001b[39m!r}\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    757\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, item):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Choice' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "summarize_dialogue (\"\"\"The first prompt is used in the getGPTResponse function and is designed specifically for the OpenAI GPT model. It consists of a context message and a user prompt, which are formatted according to the requirements of the OpenAI API. The context message sets the stage for the conversation, specifying the roles of the system and the user, while the user prompt provides the input for generating the response.\n",
    "\n",
    "The second prompt is used in a different context, specifically in the scenario where the agent mode is set to AGENT_MODE.RESEARCH.value. This prompt is designed for a different purpose and is not directly related to the OpenAI GPT model. It provides instructions and guidelines for an imaginary interviewer character, detailing how they should interact with the conversational partner in a research interview setting. This prompt includes various components such as the context, goal, adaptive goals, information about the conversation, and criteria for crafting responses. It is more elaborate and tailored to a specific conversational scenario, unlike the simpler user prompt used for generating responses with the GPT model.\n",
    "\n",
    "The difference lies in the level of detail and the intended use of each prompt. The first prompt is suitable for generating responses with the GPT model, while the second prompt serves as a set of instructions for a character in a simulated conversation.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Can you share your experience with VR chat platforms and how long have you been using them?\n",
      "2. What are some specific activities or games you enjoy participating in within VR chat environments?\n",
      "3. How has VR chat enhanced your social interactions or communication with others compared to traditional methods?\n",
      "4. Can you discuss a memorable moment or experience you had while engaging in VR chat activities?\n",
      "5. In your opinion, what unique aspects or features of VR chat make it stand out from other forms of online communication or entertainment?\n",
      "6. How do you see VR chat influencing the future of social interactions and virtual communities?\n",
      "7. Have you encountered any challenges or limitations while using VR chat, and how have you overcome them?\n",
      "8. What motivates you to r\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Original list of questions\n",
    "question_list = ['1. Can you share your experience with VR chat platforms and how long have you been using them?',\n",
    "                 '2. What are some specific activities or games you enjoy participating in within VR chat environments?',\n",
    "                 '3. How has VR chat enhanced your social interactions or communication with others compared to traditional methods?',\n",
    "                 '4. Can you discuss a memorable moment or experience you had while engaging in VR chat activities?',\n",
    "                 '5. In your opinion, what unique aspects or features of VR chat make it stand out from other forms of online communication or entertainment?',\n",
    "                 '6. How do you see VR chat influencing the future of social interactions and virtual communities?',\n",
    "                 '7. Have you encountered any challenges or limitations while using VR chat, and how have you overcome them?',\n",
    "                 '8. What motivates you to r']\n",
    "\n",
    "# Filtered list containing only questions starting with a number\n",
    "filtered_questions = [question for question in question_list if re.match(r'^\\d+\\.', question)]\n",
    "\n",
    "# Print the filtered questions\n",
    "for question in filtered_questions:\n",
    "    print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you describe your experience using VR chat platforms?\n",
      "What specific activities do you enjoy participating in while using VR chat?\n",
      "How has VR chat enhanced or influenced your social interactions compared to traditional means?\n",
      "Could you share a memorable moment or experience you had in VR chat that had a significant impact on you?\n",
      "In your opinion, what makes VR chat a valuable tool for communication and interaction?\n",
      "How do you see the future of VR chat evolving, and what role do you think you could play in that evolution?\n",
      "What features or aspects of VR chat do you believe set it apart from other forms of online communication?\n",
      "Have you encountered any challenges while using VR chat, and how have you overcome them?\n",
      "How important is customization and personalization in your VR chat experience, and why?\n",
      "In what ways do you think VR chat can positively impact society and relationships in the future?\n"
     ]
    }
   ],
   "source": [
    "def remove_numbers(question_list):\n",
    "    processed_questions = []\n",
    "    for question in question_list:\n",
    "        # Find the index of the first space after the number\n",
    "        space_index = question.find(' ')\n",
    "        # Remove the number and the following space\n",
    "        processed_question = question[space_index+1:]\n",
    "        processed_questions.append(processed_question)\n",
    "    return processed_questions\n",
    "\n",
    "# Original list of questions\n",
    "question_list = ['1. Can you describe your experience using VR chat platforms?',\n",
    "                 '2. What specific activities do you enjoy participating in while using VR chat?',\n",
    "                 '3. How has VR chat enhanced or influenced your social interactions compared to traditional means?',\n",
    "                 '4. Could you share a memorable moment or experience you had in VR chat that had a significant impact on you?',\n",
    "                 '5. In your opinion, what makes VR chat a valuable tool for communication and interaction?',\n",
    "                 '6. How do you see the future of VR chat evolving, and what role do you think you could play in that evolution?',\n",
    "                 '7. What features or aspects of VR chat do you believe set it apart from other forms of online communication?',\n",
    "                 '8. Have you encountered any challenges while using VR chat, and how have you overcome them?',\n",
    "                 '9. How important is customization and personalization in your VR chat experience, and why?',\n",
    "                 '10. In what ways do you think VR chat can positively impact society and relationships in the future?']\n",
    "\n",
    "# Remove numbers from the questions\n",
    "processed_questions = remove_numbers(question_list)\n",
    "\n",
    "# Print the processed questions\n",
    "for question in processed_questions:\n",
    "    print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
