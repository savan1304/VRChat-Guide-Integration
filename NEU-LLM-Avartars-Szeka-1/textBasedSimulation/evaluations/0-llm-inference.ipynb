{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aecb046",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 18 21:33:25 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-PCIE-32GB           Off | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   45C    P0              40W / 250W |      0MiB / 32768MiB |      4%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "2\n",
      "32\n",
      "cpu cores\t: 16\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!cat /proc/cpuinfo |grep \"physical id\"|sort |uniq|wc -l \n",
    "!cat /proc/cpuinfo |grep \"processor\"|wc -l\n",
    "!cat /proc/cpuinfo |grep \"cores\"|uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30d22358",
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 58.762919,
     "end_time": "2024-01-08T12:50:14.260138",
     "exception": false,
     "start_time": "2024-01-08T12:49:15.497219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q -U transformers\n",
    "# !pip install -q -U accelerate\n",
    "# !pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4982e48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "GPT4 = \"gpt-4\"\n",
    "API_KEY = os.environ.get(\"API_KEY\")\n",
    "openai_client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "IMPORTANT_OBSERVATIONS=\"Important Observations\"\n",
    "NPC_RESPONSE=\"NPC Response\"\n",
    "USER_MESSAGE=\"Message\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17768866",
   "metadata": {},
   "source": [
    "# preprocess, delete unncessary portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e77aac29-2409-4096-be76-416e8a1a0a64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_and_process_csv(file_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, extracts relevant columns, and returns a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed DataFrame.\n",
    "    \"\"\"\n",
    "    observation_df = pd.read_csv(file_path, index_col=False, encoding='latin-1')\n",
    "    conversation_df = observation_df[[USER_MESSAGE, NPC_RESPONSE, IMPORTANT_OBSERVATIONS]]\n",
    "    return conversation_df\n",
    "\n",
    "def read_multiple_csv_files(folder_path):\n",
    "    \"\"\"\n",
    "    Reads multiple CSV files from a folder and returns a dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary of DataFrames with file labels as keys.\n",
    "    \"\"\"\n",
    "    csv_files = [file for file in os.listdir(folder_path) if file.endswith(\".csv\")]\n",
    "    dfs = {}\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        label = os.path.splitext(csv_file)[0]\n",
    "        file_path = os.path.join(folder_path, csv_file)\n",
    "        print(file_path)\n",
    "        dfs[label] = read_and_process_csv(file_path)\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc51378",
   "metadata": {
    "papermill": {
     "duration": 0.004511,
     "end_time": "2024-01-08T12:50:15.553988",
     "exception": false,
     "start_time": "2024-01-08T12:50:15.549477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# remove decimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f203b9",
   "metadata": {
    "papermill": {
     "duration": 0.015853,
     "end_time": "2024-01-08T12:50:15.574443",
     "exception": false,
     "start_time": "2024-01-08T12:50:15.558590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# # Assuming dfs is your dictionary of DataFrames\n",
    "# for scenario, df in dfs.items():\n",
    "#     df[\"Important Observations\"] = df[\"Important Observations\"].map(lambda obs: str(obs))\n",
    "#     df[\"Important Observations\"] = df[\"Important Observations\"].apply(lambda obs: re.sub(r'^\\d+\\.', '', obs, flags=re.MULTILINE))\n",
    "# dfs[next(iter(dfs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aabfc402",
   "metadata": {
    "papermill": {
     "duration": 25.927693,
     "end_time": "2024-01-08T12:50:41.506845",
     "exception": false,
     "start_time": "2024-01-08T12:50:15.579152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 21:34:10.921876: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-18 21:34:10.922174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-18 21:34:10.929254: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-18 21:34:10.989580: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-18 21:34:25.899121: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoConfig\n",
    "from accelerate import init_empty_weights,load_checkpoint_and_dispatch\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "import torch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43fb6d8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/zhang.jinda1/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50744559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_id=\"NousResearch/Llama-2-7b-hf\"\n",
    "model_id_llama=\"meta-llama/Llama-2-13b-chat-hf\"\n",
    "model_id_mistral=\"mistralai/Mistral-7B-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b06edc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6109310b28ce4e8a8e95ff1198865c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== Load the Model\n",
    "def load_model_and_tokenizer(model_id):\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "  \n",
    "  if 'mistral' in model_id:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "  return model, tokenizer\n",
    "\n",
    "model_llama, tokenizer_llama = load_model_and_tokenizer(model_id_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1edf4c10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_mistral, tokenizer_mistral = load_model_and_tokenizer(model_id_mistral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc15c43e",
   "metadata": {
    "papermill": {
     "duration": 0.007129,
     "end_time": "2024-01-08T12:54:23.267796",
     "exception": false,
     "start_time": "2024-01-08T12:54:23.260667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  sample usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6538e56",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# generate one response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6747808",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#https://huggingface.co/docs/transformers/main_classes/text_generation\n",
    "def get_response(prompt, pipe):\n",
    "    sequences = pipe(\n",
    "        prompt,\n",
    "        do_sample=True, #Whether or not to use sampling; \n",
    "        max_new_tokens=500,\n",
    "        temperature=0.1,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    res=sequences[0]['generated_text']\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_eval_results(message, npc_response, relevant_observations, model_id):\n",
    "    if \"Mistral\" in model_id:\n",
    "        pipe=pipeline(\n",
    "            \"text-generation\", \n",
    "            model=model_mistral, \n",
    "            tokenizer=tokenizer_mistral, \n",
    "            torch_dtype=torch.bfloat16, \n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "    elif \"llama\" in model_id:\n",
    "        pipe=pipeline(\n",
    "            \"text-generation\", \n",
    "            model=model_llama, \n",
    "            tokenizer=tokenizer_llama, \n",
    "            torch_dtype=torch.bfloat16, \n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    prompt = f\"\"\"\n",
    "        You are to analyze a conversation between a user and an avatar, where the user sends a Question, and the avatar generates a response based on a list of obserif vations. Your task is to impartially judge and rate the contribution of each observation to the avatar's response on a scale of 1-5. You will be provided with the user's Question, the list of observations, and the avatar's response.\n",
    "        \n",
    "        \n",
    "        User Question: {message}\n",
    "        List of observations: {relevant_observations}\n",
    "        Avatar response: {npc_response}\n",
    "\n",
    "        For each observation in the input, provide a response in the following format: \n",
    "            Observation: Text of the original observation\n",
    "            Score: judge and rate the contribution of each observation to the avatar's response on a scale of 1-5\n",
    "            Explanation: Provide logical reasoning for contribution score between current observation to the avatar's response\n",
    "    \"\"\"\n",
    "    return get_response(prompt,pipe )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5875ad68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to get a response from the GPT model based on user input\n",
    "def get_gpt_response(prompt, gptModel):\n",
    "    # Create a chat completion using OpenAI's API\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=gptModel,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    # Extract and return the content of the first choice in the response\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def generate_eval_results_gpt(\n",
    "    message,npc_response,relevant_observations\n",
    "):\n",
    "    prompt = {\n",
    "       \"context\" : \"You are to analyze a conversation between a user and an avatar, where the user sends a message, and the avatar generates a response based on a list of observations. Your task is to impartially judge and rate the contribution of each observation to the avatar's response on a scale of 1-5. You will be provided with the user's message, the list of observations, and the avatar's response. For each observation, provide a score and a brief explanation for the score.\",\n",
    "       \"information\":f\"User message :{message}\\nList of observations : {relevant_observations}\\nAvatar response : {npc_response}\",\n",
    "       \"Example input\":\"User message: Hi John! How are you doing? How is your family?\\nList of observations: [John Lin loves his family very much., John Lin is living with his wife, Mei Lin, who is a college professor, and son, Eddy Lin, who is a student studying music theory., John Lin loves to help people.]\\n Avatar Response: Hi Katie! I'm doing well, thank you for asking. My family is doing great too. Mei Lin is busy with her teaching, and Eddy is enjoying his music theory studies. How about you? How are you doing?\",\n",
    "       \"Output criteria\": \"For each observation in the input, provide a response in the following format: Observation: {Text of the observation}\\nScore : {Provide a value between 1-5}\\n Explanation : {Provide a short reasoning for the score}\"\n",
    "    }\n",
    "    eval_prompt= json.dumps(prompt, indent=4)\n",
    "    # print(prompt)\n",
    "    return get_gpt_response(eval_prompt,GPT4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831846e3",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d59f95b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "torch.cuda.set_per_process_memory_fraction(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20623f46",
   "metadata": {},
   "source": [
    "# mistral testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d20cb7bc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# message=conversation_df[USER_MESSAGE][0]\n",
    "# observation_list=conversation_df[IMPORTANT_OBSERVATIONS][0].split('\\n')\n",
    "# npc_response=conversation_df[NPC_RESPONSE][0]\n",
    "# res = generate_eval_results(message,npc_response,observation_list,model_id_mistral)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8aca7a",
   "metadata": {},
   "source": [
    "# llama testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a3db93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message=conversation_df[USER_MESSAGE][0]\n",
    "# observation_list=conversation_df[IMPORTANT_OBSERVATIONS][0].split('\\n')\n",
    "# npc_response=conversation_df[NPC_RESPONSE][0]\n",
    "# res = generate_eval_results(message,npc_response,observation_list,model_id_llama)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84820851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "def calculate_mean_average_precision(model_id, dfs, is_gpt=False):\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file_path = f'log/output_log_{current_time}.txt'\n",
    "    sys.stdout = open(log_file_path, 'w', encoding='utf-8')\n",
    "\n",
    "    average_scores_per_scenario = {}    \n",
    "    for scenario, df in dfs.items():\n",
    "        average_scores_per_question_scenario = []  # List to store MAP for each question in the scenario\n",
    "\n",
    "        for i in range(len(df)):\n",
    "            message = df[USER_MESSAGE][i]\n",
    "\n",
    "            # Check if the value in IMPORTANT_OBSERVATIONS is a float\n",
    "            obs_value = df[IMPORTANT_OBSERVATIONS][i]\n",
    "            if isinstance(obs_value, float):\n",
    "                # Handle the case where it's a float (you might want to decide what to do in this case)\n",
    "                print(f\"Skipping scenario {scenario}, question {i + 1} due to float value in IMPORTANT_OBSERVATIONS\")\n",
    "                continue\n",
    "\n",
    "            # Convert to string and then split\n",
    "            observation_list = str(obs_value).split('\\n')\n",
    "\n",
    "            npc_response = df[NPC_RESPONSE][i]\n",
    "            \n",
    "            len_obs = len(observation_list)\n",
    "            # Skip if observation_list contains 'nan'\n",
    "            if 'nan' in observation_list:\n",
    "                print(f\"Skipping scenario {scenario}, question {i + 1} due to 'nan' in observation_list\")\n",
    "                continue\n",
    "\n",
    "            if not is_gpt:\n",
    "                res = generate_eval_results(message, npc_response, observation_list, model_id)\n",
    "            else:\n",
    "                res = generate_eval_results_gpt(message=message, npc_response=npc_response, relevant_observations=observation_list)\n",
    "\n",
    "            observations = res.split(\"\\n\\n\")\n",
    "            \n",
    "            scores = []\n",
    "            for observation in observations:\n",
    "                print(observation.encode('utf-8', 'ignore').decode('utf-8'))  # Ignore non-UTF-8 characters\n",
    "                # Use re.finditer to find all matches of the score pattern\n",
    "                matches = re.finditer(r\"Score\\s*:\\s*([\\d.]+)\", observation)\n",
    "                n = 0\n",
    "                for match in matches:\n",
    "                    if n >= len_obs:\n",
    "                        break\n",
    "                    score_str = match.group(1)\n",
    "                    score = float(score_str)\n",
    "                    scores.append(score)\n",
    "                    n += 1\n",
    "            print(scores)\n",
    "            if len(scores) > 0:\n",
    "                average_score = sum(scores) / len(scores)\n",
    "                print(f\"MAP for question {i + 1} in Scenario {scenario}: {average_score}\")\n",
    "\n",
    "            average_scores_per_question_scenario.append(average_score)\n",
    "\n",
    "        mean_average_precision_scenario = sum(average_scores_per_question_scenario) / len(average_scores_per_question_scenario)\n",
    "        average_scores_per_scenario[scenario] = mean_average_precision_scenario\n",
    "\n",
    "        print(f\"Mean Average Precision (MAP) for Whole Scenario {scenario}: {mean_average_precision_scenario}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "    # Display results for each scenario\n",
    "    for scenario, map_value in average_scores_per_scenario.items():\n",
    "        print(f\"MAP for Scenario {scenario}: {map_value}\")\n",
    "\n",
    "    # Overall Mean Average Precision across all scenarios\n",
    "    mean_average_precision = sum(average_scores_per_scenario.values()) / len(average_scores_per_scenario)\n",
    "\n",
    "    print(f\"The Overall Mean Average Precision (MAP) for {model_id} across all scenarios is: {mean_average_precision}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9da8fe8-542f-4d35-b781-a8c874d8864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder_path = \"data/r2/33\"\n",
    "# dfs = read_multiple_csv_files(data_folder_path)\n",
    "# calculate_mean_average_precision(\"GPT4\", dfs, is_gpt=True)\n",
    "# calculate_mean_average_precision(model_id_llama, dfs)\n",
    "# calculate_mean_average_precision(model_id_mistral, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa603ca1-6e7f-4d82-992e-dc6be8a5f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder_path = \"data/r2/35\"\n",
    "# dfs = read_multiple_csv_files(data_folder_path)\n",
    "# calculate_mean_average_precision(\"GPT4\", dfs, is_gpt=True)\n",
    "# calculate_mean_average_precision(model_id_llama, dfs)\n",
    "# calculate_mean_average_precision(model_id_mistral, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2050cdc9-b5b4-448c-ab84-77368fe33aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder_path = \"data/r2/55\"\n",
    "# dfs = read_multiple_csv_files(data_folder_path)\n",
    "# calculate_mean_average_precision(\"GPT4\", dfs, is_gpt=True)\n",
    "# calculate_mean_average_precision(model_id_llama, dfs)\n",
    "# calculate_mean_average_precision(model_id_mistral, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9612f52-c46f-4539-a4df-fac1365882d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder_path = \"data/r2/53\"\n",
    "# dfs = read_multiple_csv_files(data_folder_path)\n",
    "# calculate_mean_average_precision(\"GPT4\", dfs, is_gpt=True)\n",
    "# calculate_mean_average_precision(model_id_llama, dfs)\n",
    "# calculate_mean_average_precision(model_id_mistral, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9914f0b7-ce77-4266-9a52-7a916ff8c960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_folder_path = \"data/r2/57\"\n",
    "# dfs = read_multiple_csv_files(data_folder_path)\n",
    "# calculate_mean_average_precision(\"GPT4\", dfs, is_gpt=True)\n",
    "# calculate_mean_average_precision(model_id_llama, dfs)\n",
    "# calculate_mean_average_precision(model_id_mistral, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "179c04eb-8fd7-4cea-9f7c-add94ae1865f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_folder_path = \"data/r2/75\"\n",
    "# dfs = read_multiple_csv_files(data_folder_path)\n",
    "# calculate_mean_average_precision(\"GPT4\", dfs, is_gpt=True)\n",
    "# calculate_mean_average_precision(model_id_llama, dfs)\n",
    "# calculate_mean_average_precision(model_id_mistral, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2c44f-f281-4f3c-8f9b-122a7d204085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_folder_path = \"data/r3/35\"\n",
    "# dfs = read_multiple_csv_files(data_folder_path)\n",
    "# calculate_mean_average_precision(\"GPT4\", dfs, is_gpt=True)\n",
    "# calculate_mean_average_precision(model_id_llama, dfs)\n",
    "# calculate_mean_average_precision(model_id_mistral, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b7baf-28fc-41ce-bbf4-ee0191614ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder_path = \"data/r3/55\"\n",
    "# dfs = read_multiple_csv_files(data_folder_path)\n",
    "# calculate_mean_average_precision(\"GPT4\", dfs, is_gpt=True)\n",
    "# calculate_mean_average_precision(model_id_llama, dfs)\n",
    "# calculate_mean_average_precision(model_id_mistral, dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db64ff86-418a-43d8-9f26-ed091861fec6",
   "metadata": {},
   "source": [
    "# third run different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e43a36-a423-47fd-b36e-b3c5092844b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_folder_path = \"data/r3/55\"\n",
    "# dfs = read_multiple_csv_files(data_folder_path)\n",
    "# calculate_mean_average_precision(model_id_llama, dfs)\n",
    "# calculate_mean_average_precision(model_id_mistral, dfs)\n",
    "# calculate_mean_average_precision(\"GPT4\", dfs, is_gpt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284694ae-a0fd-4396-8ae9-6b315501a198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_folder_path = \"data/r3/35\"\n",
    "# dfs = read_multiple_csv_files(data_folder_path)\n",
    "# calculate_mean_average_precision(\"GPT4\", dfs, is_gpt=True)\n",
    "# calculate_mean_average_precision(model_id_llama, dfs)\n",
    "# calculate_mean_average_precision(model_id_mistral, dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b61b25-b61c-428e-8c4c-23b5cb453ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_folder_path = \"data/r3/33\"\n",
    "# dfs = read_multiple_csv_files(data_folder_path)\n",
    "# calculate_mean_average_precision(\"GPT4\", dfs, is_gpt=True)\n",
    "# calculate_mean_average_precision(model_id_llama, dfs)\n",
    "# calculate_mean_average_precision(model_id_mistral, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c059511-4ba8-46ab-9c0f-ab255dbd68e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_folder_path = \"data/r3/53\"\n",
    "# dfs = read_multiple_csv_files(data_folder_path)\n",
    "# calculate_mean_average_precision(\"GPT4\", dfs, is_gpt=True)\n",
    "# calculate_mean_average_precision(model_id_llama, dfs)\n",
    "# calculate_mean_average_precision(model_id_mistral, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fd8fc0-f8d3-4097-8fab-9cfce0da0b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder_path = \"data/r3/75\"\n",
    "dfs = read_multiple_csv_files(data_folder_path)\n",
    "calculate_mean_average_precision(\"GPT4\", dfs, is_gpt=True)\n",
    "calculate_mean_average_precision(model_id_llama, dfs)\n",
    "# calculate_mean_average_precision(model_id_mistral, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c8b6a-2365-4798-9085-2145d4242dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder_path = \"data/r3/77\"\n",
    "dfs = read_multiple_csv_files(data_folder_path)\n",
    "calculate_mean_average_precision(\"GPT4\", dfs, is_gpt=True)\n",
    "calculate_mean_average_precision(model_id_llama, dfs)\n",
    "# calculate_mean_average_precision(model_id_mistral, dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245f237c",
   "metadata": {},
   "source": [
    "# binary percentage checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf8878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# average_scores_per_question = []\n",
    "\n",
    "# for i in range(len(conversation_df)):\n",
    "#     message = conversation_df[USER_MESSAGE][i]\n",
    "#     observation_list = conversation_df[IMPORTANT_OBSERVATIONS][i].split('\\n')\n",
    "#     len_obs = len(observation_list)\n",
    "#     npc_response = conversation_df[NPC_RESPONSE][i]\n",
    "\n",
    "#     res = generate_eval_results(message, npc_response, observation_list)\n",
    "\n",
    "#     observations = res.split(\"\\n\\n\")\n",
    "\n",
    "#     scores = []\n",
    "#     for observation in observations:\n",
    "#         print(observation)\n",
    "#         # Use re.finditer to find all matches of the score pattern\n",
    "#         matches = re.finditer(r\"Score\\s*:\\s*([\\d.]+)\", observation)\n",
    "#         n = 0\n",
    "#         for match in matches:\n",
    "#             if n >= len_obs:\n",
    "#                 break\n",
    "#             print(f'match{match}')\n",
    "#             score_str = match.group(1)\n",
    "#             print(f'score_str{score_str}')\n",
    "#             score = float(score_str)\n",
    "           \n",
    "#             relevance_score = 1 if score >= 3 else 0\n",
    "#             scores.append(relevance_score)\n",
    "#             n += 1\n",
    "\n",
    "#     print(scores)\n",
    "#     average_score = sum(scores) / len(scores) if len(scores) > 0 else 0\n",
    "#     print(average_score)\n",
    "\n",
    "#     average_scores_per_question.append(average_score)\n",
    "\n",
    "# mean_average_precision = sum(average_scores_per_question) / len(average_scores_per_question)\n",
    "\n",
    "# print(f\"The Mean Average Precision (MAP) across all questions is: {mean_average_precision}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91ea602-d00b-429b-b802-88f32455217f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_mistral, tokenizer_mistral = load_model_and_tokenizer(model_id_mistral)\n",
    "data_folder_path = \"data/r3/75\"\n",
    "dfs = read_multiple_csv_files(data_folder_path)\n",
    "\n",
    "calculate_mean_average_precision(model_id_mistral, dfs)\n",
    "data_folder_path = \"data/r3/77\"\n",
    "dfs = read_multiple_csv_files(data_folder_path)\n",
    "\n",
    "calculate_mean_average_precision(model_id_mistral, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19e81d-8ce9-496a-9396-c35abdbe21f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4276214,
     "sourceId": 7361662,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 3899,
     "sourceId": 5111,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 316.597572,
   "end_time": "2024-01-08T12:54:27.391681",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-08T12:49:10.794109",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0c55ac07045142ee8fe28c98d8bdff63": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "22a3ec0c38a245b9bb9b3ce644edc32f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ff5b7102d5f04d42adbb7522e348421d",
       "placeholder": "​",
       "style": "IPY_MODEL_49def7cc11e64bdfbbd24217b90face6",
       "value": " 2/2 [03:39&lt;00:00, 104.54s/it]"
      }
     },
     "34a8d17417c2417882e373defdea115b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "429e74a2d2314180afba6c4dc12134bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0c55ac07045142ee8fe28c98d8bdff63",
       "placeholder": "​",
       "style": "IPY_MODEL_ef705a4b40274ac69680e87bf86a96a6",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "457b472ac75c49ab95e62765a597f3a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_429e74a2d2314180afba6c4dc12134bd",
        "IPY_MODEL_aa702456976a49ffb751b0e5b2c0d95f",
        "IPY_MODEL_22a3ec0c38a245b9bb9b3ce644edc32f"
       ],
       "layout": "IPY_MODEL_34a8d17417c2417882e373defdea115b"
      }
     },
     "49def7cc11e64bdfbbd24217b90face6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "aa702456976a49ffb751b0e5b2c0d95f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ac8647cbc7404e6089f60beb3dd06740",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c0fe16a1e9d241aa87c6726044701e10",
       "value": 2
      }
     },
     "ac8647cbc7404e6089f60beb3dd06740": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c0fe16a1e9d241aa87c6726044701e10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ef705a4b40274ac69680e87bf86a96a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ff5b7102d5f04d42adbb7522e348421d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
